# Beam Search 的基本思想
特点：每一步保留概率最高的 k 个候选序列（k 就是 beam size）。

## 设定
beam size = 2
起始输入为：“我喜欢”

## Step 1: 预测第一个词
模型预测下一个词：
Token	概率
吃		0.4
看		0.3
打		0.2
写		0.1

保留概率前 2 的：
“我喜欢吃”（概率 0.4）
“我喜欢看”（概率 0.3）

## Step 2: 对每个候选句子继续扩展

### 对 “我喜欢吃”：
Token	概率
苹果		0.5
面包		0.3
鸡蛋		0.2

“我喜欢吃苹果”（0.4 × 0.5 = 0.20）
“我喜欢吃面包”（0.4 × 0.3 = 0.12）
“我喜欢吃鸡蛋”（0.4 × 0.2 = 0.08）

### 对 “我喜欢看”：
Token	概率
电影		0.6
小说		0.3
手机		0.1

“我喜欢看电影”（0.3 × 0.6 = 0.18）
“我喜欢看小说”（0.3 × 0.3 = 0.09）
“我喜欢看手机”（0.3 × 0.1 = 0.03）

## Step 3: 从所有候选中选前2个
句子			    总概率
我喜欢吃苹果	    0.20
我喜欢看电影	    0.18
我喜欢吃面包	    0.12
我喜欢看小说	    0.09
我喜欢吃鸡蛋	    0.08
我喜欢看手机	    0.03

保留前2：
“我喜欢吃苹果”
“我喜欢看电影”

## 继续向下生成，直到终止符或达到最大长度

## 注意事项
- 概率的乘法问题：多个小数相乘会变得很小，因此通常使用 log 概率和 代替乘法。
- 长度惩罚（length penalty）：短句往往得分更高，因此需要惩罚太短的句子。
- beam size 越大，搜索更全面，但速度更慢
