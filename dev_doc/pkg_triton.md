## triton æ˜¯ç”± OpenAI å¼€å‘ çš„ä¸€ä¸ªé«˜æ€§èƒ½ æ·±åº¦å­¦ä¹ æ¨ç†å’Œè®­ç»ƒåŠ é€Ÿç¼–è¯‘å™¨ï¼Œä¸“é—¨è®¾è®¡ç”¨äº GPU ä¸Šçš„é«˜æ•ˆè‡ªå®šä¹‰ç®—å­ç¼–å†™å’Œç¼–è¯‘æ‰§è¡Œã€‚

å®ƒä¸æ˜¯ NVIDIA çš„ Triton Inference Serverï¼Œè€Œæ˜¯ä¸€ä¸ª Python æ¥å£çš„ GPU kernel ç¼–è¯‘æ¡†æ¶ã€‚

- å…¨ç§°ï¼šTriton (by OpenAI)
- ä½œç”¨ï¼šè®©ä½ å¯ä»¥åƒå†™ NumPy ä¸€æ ·å†™ GPU kernelï¼Œå¹¶è‡ªåŠ¨ç¼–è¯‘ä¸ºé«˜æ•ˆçš„ CUDA ä»£ç ã€‚
- ç›®æ ‡ï¼šæ›¿ä»£æ‰‹å†™ CUDAï¼Œæä¾›ç”Ÿäº§çº§æ€§èƒ½ï¼ŒåŒæ—¶æå‡å¼€å‘æ•ˆç‡ã€‚

## Triton çš„å…³é”®ç‰¹ç‚¹

| ç‰¹æ€§                    | æè¿°                                                       |
|-----------------------|----------------------------------------------------------|
| Python ç¼–å†™             | Triton kernel æ˜¯ç”¨ Python å†™çš„ DSLï¼ˆé¢†åŸŸä¸“ç”¨è¯­è¨€ï¼‰                   |
| è‡ªåŠ¨ä¼˜åŒ–                  | è‡ªåŠ¨è¿›è¡Œ thread/block åˆ†é…ã€memory coalescingã€vectorization ç­‰ä¼˜åŒ– |
| é«˜æ€§èƒ½                   | Triton å†™å‡ºçš„ kernel æ¥è¿‘ç”šè‡³è¶…è¿‡æ‰‹å†™ CUDA çš„æ€§èƒ½                      |
| æ˜“äºä½¿ç”¨                  | ä¸éœ€è¦äº†è§£ CUDA ç»†èŠ‚ï¼Œåˆå­¦è€…ä¹Ÿèƒ½å†™é«˜æ•ˆ kernel                            |
| A100/H100 ä¼˜åŒ–          | é’ˆå¯¹ç°ä»£ NVIDIA GPU åšäº†æ·±å…¥ä¼˜åŒ–                                   |
| æ”¯æŒ FP32 / FP16 / BF16 | å¸¸è§ç²¾åº¦æ¨¡å¼éƒ½æ”¯æŒï¼Œé€‚åˆ LLM ç­‰æ¨¡å‹ä¼˜åŒ–                                   |

## Triton åœ¨å·¥ä¸šç•Œçš„åº”ç”¨åœºæ™¯

- vLLM / FlashAttentionï¼štriton è¢«ç”¨äºå†™é«˜æ•ˆçš„ attention kernelã€‚
- HuggingFace Transformersï¼šéƒ¨åˆ† kernel å®ç°å¼€å§‹é›†æˆ tritonã€‚
- OpenAI è‡ªå®¶æ¨¡å‹ï¼šå†…éƒ¨å¹¿æ³›ä½¿ç”¨ Triton æ›¿ä»£æ‰‹å†™ CUDAã€‚
- æ¨¡å‹é‡åŒ– / çŸ©é˜µä¹˜æ³• / LayerNorm ç­‰åœºæ™¯ã€‚

## Triton å’Œ CUDA çš„åŒºåˆ«

| å¯¹æ¯”é¡¹   | Triton      | CUDA      |
|-------|-------------|-----------|
| ç¼–å†™æ–¹å¼  | Python      | C++       |
| å­¦ä¹ æ›²çº¿  | ä½           | é«˜         |
| é€‚åˆä»»åŠ¡  | ML/LLM é«˜æ•ˆç®—å­ | æ‰€æœ‰ GPU ä»»åŠ¡ |
| æ€§èƒ½    | æ¥è¿‘æ‰‹å†™ CUDA   | æ‰‹å†™å¯è¾¾æé™    |
| è°ƒè¯•/å¼€å‘ | æ›´å¿«          | æ›´å¤æ‚       |

## Triton å®˜æ–¹èµ„æ–™

- GitHub: https://github.com/openai/triton
- æ–‡æ¡£: https://triton-lang.org
- å®‰è£…ï¼ˆéœ€æ”¯æŒçš„ NVIDIA é©±åŠ¨å’Œ CUDAï¼‰ï¼špip install triton
- æ³¨æ„ï¼šTriton åªæ”¯æŒ NVIDIA GPUï¼ˆä¸æ”¯æŒ AMD / Intelï¼‰ï¼Œä¹Ÿè¦æ±‚ CUDA 11+ é©±åŠ¨ï¼Œä¸”å½“å‰ä»ä»¥ Linux ä¸ºä¸»ã€‚

## æ€»ç»“

Triton æ˜¯ä»€ä¹ˆï¼Ÿ
- âœ… OpenAI å‡ºå“çš„ GPU Kernel ç¼–è¯‘å™¨
- âœ… ç”¨ Python å†™é«˜æ€§èƒ½ GPU kernel
- âœ… æ€§èƒ½æ¥è¿‘ CUDAï¼Œå¼€å‘æ•ˆç‡é«˜
- âœ… å¹¿æ³›åº”ç”¨äº LLMã€Attentionã€å¼ é‡è¿ç®—åŠ é€Ÿåœºæ™¯

# å¦‚æœåœ¨ llmï¼Œä¹‹å‰å†™ cuda æ ¸å‡½æ•°ï¼Œç°åœ¨å¯ä»¥ç”¨ tritonæ¥ä½œä¸ºæ›¿ä»£ï¼Ÿ

æ˜¯çš„ï¼Œä¼šæœ‰ä»¥ä¸‹å¥½å¤„ï¼š

| CUDA Kernel åŠŸèƒ½                              | æ˜¯å¦é€‚åˆç”¨ Triton æ›¿ä»£ | è¯´æ˜                                 |
|---------------------------------------------|-----------------|------------------------------------|
| FlashAttention / RotaryEmbedding            | âœ… éå¸¸é€‚åˆ          | Triton å°¤å…¶æ“…é•¿ä¼˜åŒ–çŸ©é˜µä¹˜+maskæ“ä½œ            |
| LayerNorm / RMSNorm / BiasAdd               | âœ… å¾ˆé€‚åˆ           | Triton å¯ä»¥å†™å¾—æ›´ç®€æ´ï¼Œè¿˜èƒ½è‡ªåŠ¨åš vectorization |
| é‡åŒ– / åé‡åŒ–ï¼ˆQ4, Q8ï¼‰                            | âœ… å¤§éƒ¨åˆ†é€‚åˆ         | Triton æ”¯æŒ FP16/INT8 å†…å­˜è®¿é—®ä¼˜åŒ–         |
| Softmax / GELU / Swish ç­‰æ¿€æ´»å‡½æ•°                | âœ… é€‚åˆ            | Triton kernel ç¼–å†™æ¯” CUDA ç®€å•          |
| prefix caching / token mixing / beam search | âœ… å–å†³äºå¤æ‚åº¦        | å¦‚æœæ˜¯å¤§è§„æ¨¡å¹¶è¡Œæ“ä½œï¼ŒTriton è¡¨ç°å‡ºè‰²             |
| å›¾åƒå¤„ç†ã€çº¹ç†é‡‡æ ·ã€å…¨å±€åŒæ­¥                              | âŒ ä¸å»ºè®®           | Triton ä¸æ”¯æŒè¿™äº›å›¾å½¢ç›¸å…³æ“ä½œ                 |

## Triton æ›¿ä»£çš„ä¼˜åŠ¿

| ä¼˜åŠ¿                   | è¯´æ˜                                                    |
|----------------------|-------------------------------------------------------|
| ğŸ”§ å¼€å‘æ•ˆç‡é«˜             | ä¸ç”¨å…³æ³¨ warp/thread/block å†…å­˜æ¨¡å‹ï¼Œå†™æ³•æ›´æ¥è¿‘ NumPy               |
| ğŸš€ æ€§èƒ½æ¥è¿‘æ‰‹å†™ CUDA       | Triton è‡ªåŠ¨ä¼˜åŒ– memory coalescingã€å…±äº«å†…å­˜ã€vector åŒ–           |
| ğŸ§  å®¹æ˜“è°ƒè¯•              | å†™é”™ kernel æ—¶ï¼ŒPython æŠ¥é”™æ›´å‹å¥½ï¼Œä¸éœ€è¦ç”¨ `nsight` æˆ– `cuda-gdb`   |
| âš¡ æ”¯æŒè‡ªå®šä¹‰ blocksize æœç´¢ | Triton æ”¯æŒ autotune decoratorï¼ˆæ¯”å¦‚ `@triton.autotune`ï¼‰   |
| ğŸ¤ ä¸ PyTorch é›†æˆè‰¯å¥½    | Triton kernel èƒ½ç›´æ¥æ¥æ”¶ PyTorch Tensor çš„ `.data_ptr()` æŒ‡é’ˆ |

## æ³¨æ„äº‹é¡¹

| é™åˆ¶                       | è¯´æ˜                                             |
|--------------------------|------------------------------------------------|
| ä¸æ”¯æŒè·¨ block åŒæ­¥            | Triton åªæ”¯æŒ block å†… syncï¼Œä¸èƒ½åƒ CUDA é‚£æ ·åšå…¨å±€ barrier |
| ä¸æ”¯æŒåŠ¨æ€å…±äº«å†…å­˜                | æŸäº›å¤æ‚åŠ¨æ€åˆ†é…éœ€æ±‚å¯èƒ½å—é™                                 |
| ä»…æ”¯æŒ NVIDIA GPUï¼ˆ>= Voltaï¼‰ | AMD/Intel GPU æ— æ³•ä½¿ç”¨ï¼Œä¸”éœ€è¦ CUDA >= 11 é©±åŠ¨ç¯å¢ƒ         |
| éƒ¨åˆ† PyTorch æ„å»ºå·¥å…·é“¾å†²çª       | æœ‰æ—¶ Triton å®‰è£…ä¸ PyTorch ç¼–è¯‘é€‰é¡¹æœ‰å…³ï¼Œéœ€æ³¨æ„ç¯å¢ƒå…¼å®¹           |

## ä¸¾ä¾‹ï¼šFlashAttention æ›¿ä»£

- OpenAI çš„ Triton FlashAttention å®ç° æ¯” CUDA å®ç°çŸ­ 10 å€ï¼Œè¿˜èƒ½è‡ªåŠ¨ vectorizeã€‚

```python
@triton.jit
def flash_attention(Q_ptr, K_ptr, V_ptr, Out_ptr, ...):
    # è¯»å– QKV block
    q = tl.load(Q_ptr + offset)
    k = tl.load(K_ptr + offset)
    v = tl.load(V_ptr + offset)

    # åš attention score å’Œ softmax
    score = tl.dot(q, k, trans_b=True)
    score = score * scale
    score = tl.softmax(score, axis=-1)

    # å¾—åˆ°è¾“å‡º
    out = tl.dot(score, v)
    tl.store(Out_ptr + offset, out)
```